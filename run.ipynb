{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n# nom_convert_int \\n    update it in the software \\n    \\n# split time changes so please change it also\\n\\n\\n# update your code to take care of both csv and excel file  \\n    \\n    \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jul 30 13:48:23 2017\n",
    "\n",
    "@author: Muddassar Sharif\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import cPickle as pickle\n",
    "from datetime import datetime\n",
    "import math\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import datetime\n",
    "#from multiple_files import *\n",
    "import io\n",
    "\n",
    "class preprocess():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.address= None\n",
    "        self.list_variables=[]\n",
    "        self.list_s_variables=[]\n",
    "        self.y_var=None\n",
    "        self.mean=[]\n",
    "        self.st_dev=[]\n",
    "        self.io_dim=[[],[]]\n",
    "        self.data_array=[]\n",
    "        \n",
    "        self.data= 0  \n",
    "        self.y=None\n",
    "        self.key={}\n",
    "        self.convert=False\n",
    "        self.train_x=[]\n",
    "        self.train_x=[]\n",
    "        self.train_y=[]\n",
    "        self.test_y=[]\n",
    "        self.pre_x=[]\n",
    "        self.pre_y=[]\n",
    "        self.time= None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def file_address(self, address):\n",
    "        self.address= address\n",
    "        \n",
    "        \n",
    "    def save_data_and_y(self):\n",
    "        basename = \"preprocessing/preprocessing\"\n",
    "        suffix = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "        file_name = \"_\".join([basename, suffix]) + \".pickle\"  # e.g. 'mylogfile_120508_171442'\n",
    "        self.preprocess_p_file = file_name \n",
    "        self.data_array.append(self.preprocess_p_file)\n",
    "        \n",
    "        with open(self.data_array[-1], 'wb') as f:\n",
    "            pickle.dump((self.data, self.y), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "            \n",
    "        # time to empty the spaces \n",
    "        self.data=0\n",
    "        self.y= None\n",
    "        self.train_x=[]\n",
    "        self.train_x=[]\n",
    "        self.train_y=[]\n",
    "        self.test_y =[]\n",
    "        \n",
    "            \n",
    "    def load_data_and_y(self, i):\n",
    "        with open(self.data_array[i], 'rb') as f:\n",
    "            self.data, self.y= pickle.load(f)\n",
    "\n",
    "        \n",
    "        \n",
    "    def read_file(self, nrows, start):   # needs modification\n",
    "        try:\n",
    "            if start== 0:\n",
    "                \n",
    "                if nrows== None:\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        self.data= pd.read_csv(self.address)\n",
    "                    except:\n",
    "                        self.data = pd.read_excel(self.address)\n",
    "\n",
    "                    return len(self.data.index)\n",
    "                else:\n",
    "                    self.data= pd.read_csv(self.address, nrows= nrows)\n",
    "                    return len(self.data.index)\n",
    "            else:\n",
    "                self.data= pd.read_csv(self.address)\n",
    "                index= len(self.data.index)\n",
    "                self.data = self.data[start:]\n",
    "                return index\n",
    "                \n",
    "        except:\n",
    "            print(\"File does not exist\")\n",
    "            return 0\n",
    "    \n",
    "    def pandas_data(self):\n",
    "        return self.data\n",
    "          \n",
    "    def variables(self):\n",
    "        self.list_variables= self.data.columns.values.tolist()\n",
    "        self.list_s_variables= self.list_s_variables\n",
    "        return self.list_variables\n",
    "            \n",
    "    def extract_variables(self, variables):\n",
    "        if variables == None:\n",
    "            if self.list_s_variables==[]:\n",
    "                \n",
    "                self.list_s_variables= np.array(self.list_variables)\n",
    "        else:\n",
    "            self.list_s_variables= np.array(variables)\n",
    "        \n",
    "        if self.convert== True:\n",
    "            self.data=self.data[np.append(self.list_s_variables, self.y_var)]\n",
    "        return self.list_s_variables\n",
    "    \n",
    "    def replace_nan(self, nan):\n",
    "        try:\n",
    "            \n",
    "            self.data= self.data.fillna(nan)\n",
    "            return \" replace nan done!\"\n",
    "        except:\n",
    "            print(\"sorry an error occured!\" )\n",
    "            return \" Sorry replace nan not done\"\n",
    "        \n",
    "    \n",
    "    def split_time(self):\n",
    "        self.time= None\n",
    "        temp=[]\n",
    "        for col in self.list_s_variables: \n",
    "            if self.data[col].dtype == 'object':\n",
    "                try:\n",
    "                    temp2 = pd.to_datetime(self.data[col][:10])\n",
    "                    temp.append(col)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        \n",
    "        if temp== []:\n",
    "            return \"files does not have the time attribute\"\n",
    "        else:\n",
    "            self.time= temp[0]\n",
    "            temp= pd.to_datetime(self.data[self.time])\n",
    "            \n",
    "             \n",
    "     \n",
    "            self.data[self.time+ 'yearrr']= temp.dt.year\n",
    "            self.data[self.time+ 'monthhh']= temp.dt.month\n",
    "            self.data[self.time + 'dayyy']= temp.dt.day\n",
    "            if self.convert== False:  # limit to running only the first time\n",
    "            \n",
    "                self.list_s_variables= np.append(self.list_s_variables, self.time+ 'yearrr')\n",
    "                self.list_s_variables= np.append(self.list_s_variables, self.time+ 'monthhh')\n",
    "                self.list_s_variables= np.append(self.list_s_variables, self.time+ 'dayyy')\n",
    "                self.list_variables= np.append(self.list_variables, self.time+ 'yearrr')\n",
    "                self.list_variables= np.append(self.list_variables, self.time+ 'monthhh')\n",
    "                self.list_variables= np.append(self.list_variables, self.time+ 'dayyy')\n",
    "                \n",
    "                i= np.where(self.list_s_variables==self.time)\n",
    "                self.list_s_variables= np.delete(self.list_s_variables, i[0][0], None)\n",
    "                \n",
    "    \n",
    "            self.data=self.data.drop(self.time, axis=1)\n",
    "            \n",
    "    \n",
    "            return \"removed and splitted \" +self.time +\"!\"\n",
    "    \n",
    "    \n",
    "    def delete_row(self,row):\n",
    "        try:\n",
    "            self.data=self.data.drop(row, axis=1) \n",
    "            i= np.where(self.list_s_variables==row)\n",
    "            self.list_s_variables= np.delete(self.list_s_variables, i[0][0], None)\n",
    "            return \"row \" +str(row) +\" deleted\"\n",
    "        except:\n",
    "            return \"row \" +str(row) + \" does not exist\"\n",
    "        \n",
    "    def Filterout(self, row, value):\n",
    "        try:\n",
    "            self.data = self.data.loc[self.data[row]!=value]\n",
    "            return \"data only contain rows with: \" + str(row) +\" != \" + str(value)\n",
    "        except:\n",
    "            return \"please check your inputs please. Filteration not done\"\n",
    "        \n",
    "    def Filterout_product(self, row, value, y):\n",
    "        try:\n",
    "            self.pre_x = self.data.loc[self.data[row]==value]\n",
    "            self.data = self.data.loc[self.data[row]!=value]\n",
    "\n",
    "            self.pre_y= np.array(self.pre_x[y])\n",
    "        \n",
    "            self.pre_x=self.pre_x.drop(y, axis=1)\n",
    "            return \"data for prediction containing rows with: \" + str(row) +\" != \" + str(value)\n",
    "        except:\n",
    "            return \"please check your inputs please. Filteration of new product data not done\"\n",
    "     \n",
    "    def Filterin(self, row, value):\n",
    "        try:\n",
    "            self.data = self.data.loc[self.data[row]==value]\n",
    "            return \"data only contain rows with: \" + str(row) +\"==\" + str(value)\n",
    "        except:\n",
    "            return \"please check your inputs please. Filteration not done\"\n",
    "        \n",
    "    def Binning(self, inp):# not tested yet\n",
    "        try:\n",
    "            \n",
    "            if inp==None:\n",
    "                for x in self.list_s_variables:\n",
    "                    if (type(self.data[x][1])== np.int64 ) or (type(self.data[x][1])== np.float64 ):\n",
    "        #                    #binning when required\n",
    "                            if (self.data[[x]].max()-self.data[[x]].min()) > 40:\n",
    "                                length= 20\n",
    "                                group_names = [i for i in range(1,length+1)]\n",
    "                                jump= (self.data[[x]].max()-self.data[[x]].min())/length+ (self.data[[x]].max()-self.data[[x]].min())%length\n",
    "                                number=self.data[[x]].min()                \n",
    "                                bins=[]\n",
    "                                for j in range(len(length)+1):              \n",
    "                                    bins.append(number)\n",
    "                                    number= number+jump\n",
    "                                \n",
    "                                bins[0]= bins[0]-10\n",
    "                                bins[-1]= bins[-1] + 10\n",
    "                                self.data[x] = pd.cut(self.data[x], bins, labels=group_names)\n",
    "                    else:\n",
    "                        pass\n",
    "            \n",
    "            else:\n",
    "                if (self.data[[inp]].max()-self.data[[inp]].min()) > 40:\n",
    "                    length= 20\n",
    "                    group_names = [i for i in range(1,length+1)]\n",
    "                    jump= (self.data[[x]].max()-self.data[[x]].min())/length+ (self.data[[x]].max()-self.data[[x]].min())%length\n",
    "                    number=self.data[[x]].min()                \n",
    "                    bins=[]\n",
    "                    for j in range(len(length)+1):              \n",
    "                        bins.append(number)\n",
    "                        number= number+jump\n",
    "                                \n",
    "                        bins[0]= bins[0]-10\n",
    "                        bins[-1]= bins[-1] + 10\n",
    "                        self.data[x] = pd.cut(self.data[x], bins, labels=group_names)\n",
    "            return \"Binning Done!\"\n",
    "        except:\n",
    "            return \"An error occured when Binning\"\n",
    "        \n",
    "        \n",
    "    def category_to_nominal(self):\n",
    "        if self.convert==False:\n",
    "            self.convert= self.make_key()\n",
    "        \n",
    "        for x in self.list_s_variables:\n",
    "\n",
    "            if (type(np.array(self.data[x])[1])== np.int64 ) or (type(np.array(self.data[x])[1])== np.float64 ) or (type(np.array(self.data[x])[1])== long ):\n",
    "#                    std= self.data[[x]].std()\n",
    "#                    mean= self.data[[x]].mean()\n",
    "#                    self.data[[x]]=np.tan(np.array(((self.data[[x]]-mean)/std)*.01 +1))\n",
    "                pass\n",
    "            else:\n",
    "                self.data[[x]]=self.nom_convert_int(self.data[[x]],x)\n",
    "              \n",
    "        return \"categoricol to nominal done!\"\n",
    "            \n",
    "    def y_to_float(self, y):\n",
    "        try:\n",
    "            self.data[y]= np.array(self.data[y].str.replace(\",\", \"\").astype(float))\n",
    "            return \" y converted to float \"\n",
    "        \n",
    "        except:\n",
    "            return \" Error while converting y to float\"\n",
    "        \n",
    "        \n",
    "    def choose_y(self, y):\n",
    "        temp= self.y_to_float(y)\n",
    "        self.y_var= y\n",
    "        self.y= np.array(self.data[y])\n",
    "        \n",
    "        self.data=self.data.drop(y, axis=1)\n",
    "        \n",
    "        if self.convert== False:\n",
    "            i= np.where(self.list_s_variables==y)\n",
    "    \n",
    "            self.list_s_variables= np.delete(self.list_s_variables, i[0][0], None)\n",
    "        \n",
    "        return \"y choosen\"\n",
    "    \n",
    "    def normalize(self):\n",
    "#        self.mean\n",
    "#        self.st_dev\n",
    "        return True\n",
    "\n",
    "    \n",
    "    def calculate_dim(self):\n",
    "        for x in self.list_s_variables:\n",
    "            temp=(int(max(len(self.data[x].unique()), self.data[x].max()))+5)\n",
    "            self.io_dim[0].append(temp)\n",
    "            if temp>= 8:   \n",
    "                self.io_dim[1].append(int(temp*.025+3))\n",
    "            else:\n",
    "                self.io_dim[1].append(temp-1)\n",
    "                \n",
    "        return \"dimensions calculated\"\n",
    "        \n",
    "    def test_train_divide(self, l):\n",
    "        self.train_x , self.test_x, self.train_y ,self.test_y= train_test_split(np.array(self.data),np.array(self.y), test_size=l)\n",
    "    \n",
    "    \n",
    "    def get_train_x(self):\n",
    "        return self.train_x\n",
    "    def get_test_x(self):\n",
    "        return self.test_x        \n",
    "    def get_train_y(self):\n",
    "        return self.train_y  \n",
    "    def get_test_y(self):\n",
    "        return self.test_y\n",
    "      \n",
    "    def get_x(self):\n",
    "        return self.data\n",
    "    \n",
    "    def get_y(self):\n",
    "        return self.y\n",
    "    \n",
    "    def get_pre_x(self):\n",
    "        return self.pre_x\n",
    "    \n",
    "    def get_pre_y(self):\n",
    "        return self.pre_y \n",
    "        \n",
    "    \n",
    "    def get_y_var(self):\n",
    "        return self.y_var\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return self.io_dim\n",
    "    \n",
    "    def get_key(self):\n",
    "        return self.key\n",
    "    \n",
    "    def get_s_variables(self):\n",
    "        return self.list_s_variables\n",
    "    \n",
    "    def get_time_var_name(self):\n",
    "        return self.time\n",
    "    \n",
    "    def set_key(self, key):\n",
    "        self.key= key\n",
    "        self.convert=True\n",
    "\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def split_data(self, split_rate):\n",
    "        return True\n",
    "        #store split data somewhere\n",
    "     \n",
    "    def save_file(self, path):\n",
    "        self.data.to_pickle(path, compression='infer')\n",
    "        return path\n",
    "        #mongodb vs pickle for now\n",
    "        # how about saving pickle file in mongodb\n",
    "        \n",
    "        \n",
    "    def nom_convert_int(self,df,x):     \n",
    "        import numpy as np\n",
    "        counter=0\n",
    "        x_c= np.array(df)\n",
    "        dic=self.key[x]\n",
    "        for i in range(len(x_c)):\n",
    "            \n",
    "            try:\n",
    "                x_c[i][0]= int(x_c[i][0])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if x_c[i][0] in dic:\n",
    "                pass\n",
    "            else:\n",
    "                dic[x_c[i][0]]=counter\n",
    "                counter=counter+1\n",
    "        for j in range(len(x_c)):\n",
    "             x_c[j][0]=dic[x_c[j][0]]\n",
    "        \n",
    "        self.key[x]=dic\n",
    "        return x_c\n",
    "    \n",
    "    \n",
    "    def make_key(self):\n",
    "        for i in self.list_s_variables:\n",
    "            self.key[i]={}\n",
    "            \n",
    "        self.convert= True\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def get_key_for_perdiction(self):\n",
    "        \n",
    "        return [self.list_s_variables, self.key]\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# nom_convert_int \n",
    "    update it in the software \n",
    "    \n",
    "# split time changes so please change it also\n",
    "\n",
    "\n",
    "# update your code to take care of both csv and excel file  \n",
    "    \n",
    "    \n",
    "\"\"\"    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from models import *\n",
    "from graph import *\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "import datetime\n",
    "import json\n",
    "import dill\n",
    "import plotly\n",
    "from plotly import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "class Dashboard():\n",
    "\n",
    "    def __init__(self, address, y_var, t,m):\n",
    "        self.data_address= address\n",
    "        self.t=t\n",
    "        self.m=m\n",
    "#        self.models={'LinearModel':[LinearModel() for i in range(3)],'RF':[RF() for i in range(3)], 'SVM': [SVM() for i in range(3)], 'XGBoost':[XGBoost() for i in range(3)], 'HistricalMedian':[HistricalMedian() for i in range(3)], 'KNN':[KNN() for i in range(3)], 'NN_with_EntityEmbedding': [NN_with_EntityEmbedding() for i in range(3)], 'NN2_with_EntityEmbedding': [NN2_with_EntityEmbedding() for i in range(3)], 'NN': [NN() for i in range(3)] }\n",
    "        self.models={'NN_with_EntityEmbedding': [NN_with_EntityEmbedding() for i in range(3)]}\n",
    "  #      self.models={'NN_with_EntityEmbedding': [NN_with_EntityEmbedding() for i in range(3)], 'Xgboost': [XGBoost() for i in range(3)]}\n",
    "#   'NN_with_EntityEmbedding': [NN_with_EntityEmbedding() for i in range(3)], 'xboost': [XGBoost() for i in range(3)]\n",
    "        self.best_model={}\n",
    "        self.y_variable=y_var\n",
    "        basename = \"preprocessing/preprocessing_object\"\n",
    "        suffix = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "        file_name = \"_\".join([basename, suffix]) + \".pickle\"  # e.g. 'mylogfile_120508_171442'\n",
    "        self.preprocess_p_file = file_name\n",
    "        suffix = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "        file_name = \"_\".join(['bestmodel/best_model', suffix]) + \".yaml\"  # e.g. 'mylogfile_120508_171442'\n",
    "        self.best_model_file= file_name\n",
    "        self.preprocess_object= preprocess()\n",
    "        self.x=None\n",
    "        self.preprocess_object.file_address(address)\n",
    "        self.mode= \"train\"\n",
    "        self.rows_info= {\"start\":0, \"end\":0}\n",
    "        self.graph_objects=[]\n",
    "        self.key= None\n",
    "        \n",
    "        \n",
    "    #save and load preprocess object \n",
    "    def save_preprocess_object(self):\n",
    "        with open(self.preprocess_p_file, 'wb') as f:\n",
    "            pickle.dump((self.preprocess_object), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def load_preprocess_object(self):\n",
    "        with open(self.preprocess_p_file, 'rb') as f:\n",
    "            self.preprocess_object= pickle.load(f)\n",
    "            \n",
    "            \n",
    "    def check_for_new_data(self):\n",
    "        \n",
    "#        try:\n",
    "            if len(pd.read_csv(self.data_address).index)== self.rows_info[\"end\"]:\n",
    "                print(\"No New Data added to the file\")\n",
    "            \n",
    "            else:\n",
    "            # time to train this baby : incremental learning \n",
    "                print(len(pd.read_csv(self.data_address).index), self.rows_info[\"end\"])\n",
    "                print(\"file has new data\")\n",
    "                self.new_data_transformation()\n",
    "#        except:\n",
    "#            print(\"some error with the file so retraining can not be done\")\n",
    "\n",
    "    \n",
    "    def test_train_switch(self, mde):\n",
    "        self.mode= mde\n",
    "        \n",
    "    def more_data(self, address):\n",
    "        self.data_address.append(address)\n",
    "        \n",
    "    def file_transformation(self):\n",
    "        \n",
    "        print(\"reading file\")\n",
    "\n",
    "        temp= self.preprocess_object.read_file(None, self.rows_info[\"start\"])  # just reading 10000 rows for testing right now\n",
    "        self.rows_info[\"end\"]= temp\n",
    "      \n",
    "        var= self.preprocess_object.variables()\n",
    "        print(var)\n",
    "\n",
    "        sel_var= self.preprocess_object.extract_variables(None)\n",
    "        print(sel_var) \n",
    "        \n",
    "        ans= self.preprocess_object.replace_nan(0)\n",
    "        print(ans)\n",
    "        \n",
    "        ans= self.preprocess_object.split_time()  # automate this process finding the data using\n",
    "        print(ans) \n",
    "    \n",
    "        ans= self.preprocess_object.Filterout_product(self.t,self.m, self.y_variable)\n",
    "        print(ans)\n",
    "        \n",
    "        if self.mode == \"train\":  \n",
    "            ans= self.preprocess_object.Filterout(self.y_variable, 0)\n",
    "            ans= self.preprocess_object.choose_y(self.y_variable)\n",
    "\n",
    "            print(ans)\n",
    "            \n",
    "#         ans= self.preprocess_object.Filterout_product(\"STYLE #\",'Style #1', self.y_variable)\n",
    "#         print(ans)        \n",
    "            \n",
    "      \n",
    "        ans1= self.preprocess_object.category_to_nominal()\n",
    "        print(ans1)\n",
    "        dim= self.preprocess_object.calculate_dim()\n",
    "        print(dim)\n",
    "#         value= self.preprocess_object.get_key()['STYLE #']['Style #3']\n",
    "#         print(value)\n",
    "        \n",
    "#         ans= self.preprocess_object.Filterout_product(\"STYLE #\",value, self.y_variable)\n",
    "#         print(ans)\n",
    "\n",
    "#         if self.mode == \"train\":  \n",
    "#             ans= self.preprocess_object.Filterout(self.y_variable, 0)\n",
    "#             ans= self.preprocess_object.choose_y(self.y_variable)\n",
    "#             print(ans)\n",
    "\n",
    "        \n",
    "        self.preprocess_object.test_train_divide(.05)\n",
    "        self.x= self.preprocess_object.get_test_x()\n",
    "        \n",
    "\n",
    "        \n",
    "    # function to take care of training when new data comes in\n",
    "    def new_data_transformation(self):\n",
    "        \n",
    "        self.save_preprocess_object()\n",
    "        print(\"reading file\")\n",
    "        self.preprocess_object.save_data_and_y()    # this will backup previous data and y\n",
    "        temp= self.preprocess_object.read_file(None, self.rows_info[\"end\"])    # reading new data\n",
    "        \n",
    "        \n",
    "        \n",
    "        ans= self.preprocess_object.split_time()  # time variable taken care of changed from self.preprocess_object.get_time_var_name()\n",
    "        print(ans) \n",
    "\n",
    "        sel_var= self.preprocess_object.extract_variables(self.preprocess_object.get_s_variables())  # this will give an errot right now\n",
    "        print(sel_var) \n",
    "        \n",
    "        ans= self.preprocess_object.replace_nan(0)\n",
    "        print(ans)\n",
    "        \n",
    "\n",
    "        \n",
    "        if self.mode == \"train\":  \n",
    "            ans= self.preprocess_object.Filterout(self.y_variable, 0)\n",
    "            ans= self.preprocess_object.choose_y(self.y_variable)\n",
    "\n",
    "            print(ans)\n",
    "            \n",
    "            # preprocess function to divide the time between test and train\n",
    "            \n",
    "      \n",
    "        ans1= self.preprocess_object.category_to_nominal()                               # checked for errors and is working properly.\n",
    "        print(ans1)\n",
    "\n",
    "        self.preprocess_object.test_train_divide(.01)                                    # no need for this step as we are just training\n",
    "        self.save_preprocess_object()\n",
    "        self.train_best_model()\n",
    "        \n",
    "        self.rows_info[\"end\"] = temp\n",
    "        \n",
    "\n",
    "    def train_best_model(self):\n",
    "        for model_name in self.best_model.keys():\n",
    "                self.best_model[model_name][0].train()    # make a traiining function to just train the file\n",
    "        \n",
    "    \n",
    "\n",
    "   \n",
    "    def finding_best_model(self):\n",
    "        self.save_preprocess_object()\n",
    "        if self.best_model.keys()!=[]:\n",
    "            for model in self.best_model.keys():\n",
    "                print('fitting'+ str(model))\n",
    "                for i in range(len(self.best_model[model])):\n",
    "                    self.best_model[model][i].input(self.preprocess_object)\n",
    "        \n",
    "        else:\n",
    "            for model in self.models.keys():\n",
    "                for i in range(len(self.models[model])):\n",
    "                    print('fitting'+ str(model))\n",
    "                    self.models[model][i].input(self.preprocess_object)\n",
    "                    \n",
    "            self.update_best_model()\n",
    "\n",
    "\n",
    "    def update_best_model(self):\n",
    "        temp= [None,None, 1000000000000000000000000000000]\n",
    "        for i in self.models.keys():\n",
    "            for model in self.models[i]:\n",
    "                if model.get_results()<temp[2]:\n",
    "                    temp=[i, model, model.get_results]\n",
    "        self.best_model[temp[0]]=[temp[1]]\n",
    "        self.preprocess_object.save_data_and_y()\n",
    "        #self.save_preprocess_object()        \n",
    "        #self.preprocess_object= None   # now added to reduce the burden on the code\n",
    "\n",
    "    def get_prediction_key(self):\n",
    "        #self.load_preprocess_object()\n",
    "        self.key= self.preprocess_object.get_key_for_perdiction()\n",
    "        return [-1, self.key[0], self.key[1]]\n",
    "#        graph1= graph()\n",
    "#        self.graph_objects.append(graph1) \n",
    "#        self.graph_objects[-1].key_from_user(key)\n",
    "#        return [self.graph_objects.index(self.graph_objects[-1]), key[0], key[1]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def make_graph_object(self):\n",
    "#        self.load_preprocess_object()\n",
    "#        key= self.preprocess_object.get_key_for_perdiction()\n",
    "        graph1= graph()\n",
    "        self.graph_objects.append(graph1) \n",
    "        self.graph_objects[-1].key_from_user(self.key)\n",
    "        return [self.graph_objects.index(self.graph_objects[-1])]\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    def make_graph(self,k, user_in):\n",
    "        if user_in != None:\n",
    "            \n",
    "            self.graph_objects[k].user_input(user_in)\n",
    "       # return self.graph_objects[k].get_df_for_per()\n",
    "        y= self.best_model[self.best_model.keys()[0]][0].guess(np.array(self.graph_objects[k].get_df_for_per()))  \n",
    "        self.graph_objects[k].set_per_y(y)\n",
    "        return [self.graph_objects.index(self.graph_objects[k]), self.graph_objects[k].get_plot_data()]\n",
    "\n",
    "\n",
    "\n",
    "    def get_best_model(self):\n",
    "        return self.best_model\n",
    "    \n",
    "    def get_trained_models(self):\n",
    "        return self.models\n",
    "    \n",
    "    def get_preporcessed_file(self):\n",
    "        return self.preprocess_object\n",
    "\n",
    "    def save(self):\n",
    "        #save the best model\n",
    "        #s= json.dumps(self.best_model)\n",
    "\n",
    "        y = self.best_model[self.best_model.keys()[0]][0].guess(self.preprocess_object.get_test_x())\n",
    "        print(y)\n",
    "\n",
    "        plotly.tools.set_credentials_file(username='ms8909', api_key='OOQ413hzFuXQFdeEbpJK')\n",
    "\n",
    "        x_temp = []\n",
    "        for i in range(len(y)):\n",
    "            x_temp.append(i)\n",
    "        data = []\n",
    "        trace1 = go.Scatter(\n",
    "            x=x_temp,\n",
    "            y=y,\n",
    "            mode='lines',\n",
    "            name='predicted Sales'\n",
    "        )\n",
    "        data.append(trace1)\n",
    "\n",
    "        trace2 = go.Scatter(\n",
    "            x=x_temp,\n",
    "            y=self.preprocess_object.get_test_y(),\n",
    "            mode='lines',\n",
    "            name='actual'\n",
    "        )\n",
    "\n",
    "        data.append(trace2)\n",
    "\n",
    "        fig = dict(data=data)\n",
    "        py.iplot(fig, filename='line-mode')\n",
    "        with open(self.best_model_file, 'w') as f:\n",
    "            dill.dump(self.best_model, f)\n",
    "            #pickle.dump((self.best_model), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        return self.best_model[self.best_model.keys()[0]][0].get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please specify file_name and the predicting variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object= Dashboard('Aaron_Data.csv', 'UnitsShipped', \"STYLE #\",'Style #1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file\n",
      "['Division', 'Season', 'Product Type', 'Customer Type Description', 'Customer #', 'Label (Style Mst)', 'Label Desc. (Order Detail)', 'Category Description', 'STYLE #', 'UnitsShipped', 'Order Type Desc', 'Date Start']\n",
      "['Division' 'Season' 'Product Type' 'Customer Type Description'\n",
      " 'Customer #' 'Label (Style Mst)' 'Label Desc. (Order Detail)'\n",
      " 'Category Description' 'STYLE #' 'UnitsShipped' 'Order Type Desc'\n",
      " 'Date Start']\n",
      " replace nan done!\n",
      "removed and splitted Date Start!\n",
      "data for prediction containing rows with: STYLE # != Style #1\n",
      "y choosen\n",
      "categoricol to nominal done!\n",
      "dimensions calculated\n"
     ]
    }
   ],
   "source": [
    "file_object.file_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fittingNN_with_EntityEmbedding\n",
      "Train on 143697 samples, validate on 7563 samples\n",
      "Epoch 1/20\n",
      "143232/143697 [============================>.] - ETA: 0s - loss: 0.0105Epoch 00001: val_loss improved from inf to 0.00815, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 79us/step - loss: 0.0105 - val_loss: 0.0081\n",
      "Epoch 2/20\n",
      "143232/143697 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00002: val_loss improved from 0.00815 to 0.00746, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 76us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 3/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0067Epoch 00003: val_loss improved from 0.00746 to 0.00723, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 4/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0062Epoch 00004: val_loss improved from 0.00723 to 0.00694, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0062 - val_loss: 0.0069\n",
      "Epoch 5/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0057Epoch 00005: val_loss improved from 0.00694 to 0.00683, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0057 - val_loss: 0.0068\n",
      "Epoch 6/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0053Epoch 00006: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0053 - val_loss: 0.0069\n",
      "Epoch 7/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0049Epoch 00007: val_loss improved from 0.00683 to 0.00659, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0049 - val_loss: 0.0066\n",
      "Epoch 8/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0047Epoch 00008: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 73us/step - loss: 0.0047 - val_loss: 0.0066\n",
      "Epoch 9/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0044Epoch 00009: val_loss improved from 0.00659 to 0.00655, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0044 - val_loss: 0.0065\n",
      "Epoch 10/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0043Epoch 00010: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0043 - val_loss: 0.0066\n",
      "Epoch 11/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0041Epoch 00011: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0041 - val_loss: 0.0066\n",
      "Epoch 12/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0039Epoch 00012: val_loss improved from 0.00655 to 0.00644, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0039 - val_loss: 0.0064\n",
      "Epoch 13/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0038Epoch 00013: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 73us/step - loss: 0.0038 - val_loss: 0.0066\n",
      "Epoch 14/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0037Epoch 00014: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0037 - val_loss: 0.0065\n",
      "Epoch 15/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0036Epoch 00015: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0036 - val_loss: 0.0065\n",
      "Epoch 16/20\n",
      "143232/143697 [============================>.] - ETA: 0s - loss: 0.0035Epoch 00016: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0035 - val_loss: 0.0065\n",
      "Epoch 17/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0034Epoch 00017: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0034 - val_loss: 0.0066\n",
      "Epoch 18/20\n",
      "143232/143697 [============================>.] - ETA: 0s - loss: 0.0034Epoch 00018: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0034 - val_loss: 0.0065\n",
      "Epoch 19/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0033Epoch 00019: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 75us/step - loss: 0.0033 - val_loss: 0.0066\n",
      "Epoch 20/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0032Epoch 00020: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0032 - val_loss: 0.0066\n",
      "('Result on validation data: ', 1.8123737423490669)\n",
      "fittingNN_with_EntityEmbedding\n",
      "Train on 143697 samples, validate on 7563 samples\n",
      "Epoch 1/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0107Epoch 00001: val_loss improved from inf to 0.00880, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 12s 80us/step - loss: 0.0107 - val_loss: 0.0088\n",
      "Epoch 2/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00002: val_loss improved from 0.00880 to 0.00759, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 3/20\n",
      "143104/143697 [============================>.] - ETA: 0s - loss: 0.0069Epoch 00003: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0069 - val_loss: 0.0076\n",
      "Epoch 4/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0063Epoch 00004: val_loss improved from 0.00759 to 0.00708, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 5/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0058Epoch 00005: val_loss improved from 0.00708 to 0.00692, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0058 - val_loss: 0.0069\n",
      "Epoch 6/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0054Epoch 00006: val_loss improved from 0.00692 to 0.00683, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0054 - val_loss: 0.0068\n",
      "Epoch 7/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0050Epoch 00007: val_loss improved from 0.00683 to 0.00676, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0050 - val_loss: 0.0068\n",
      "Epoch 8/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0047Epoch 00008: val_loss improved from 0.00676 to 0.00663, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0047 - val_loss: 0.0066\n",
      "Epoch 9/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0045Epoch 00009: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0045 - val_loss: 0.0067\n",
      "Epoch 10/20\n",
      "143232/143697 [============================>.] - ETA: 0s - loss: 0.0043Epoch 00010: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0043 - val_loss: 0.0067\n",
      "Epoch 11/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0041Epoch 00011: val_loss improved from 0.00663 to 0.00652, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0041 - val_loss: 0.0065\n",
      "Epoch 12/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0040Epoch 00012: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0040 - val_loss: 0.0066\n",
      "Epoch 13/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0038Epoch 00013: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0038 - val_loss: 0.0065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0037Epoch 00014: val_loss improved from 0.00652 to 0.00649, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0037 - val_loss: 0.0065\n",
      "Epoch 15/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0036Epoch 00015: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 75us/step - loss: 0.0036 - val_loss: 0.0067\n",
      "Epoch 16/20\n",
      "143104/143697 [============================>.] - ETA: 0s - loss: 0.0035Epoch 00016: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0035 - val_loss: 0.0065\n",
      "Epoch 17/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0034Epoch 00017: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0034 - val_loss: 0.0066\n",
      "Epoch 18/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0033Epoch 00018: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0033 - val_loss: 0.0066\n",
      "Epoch 19/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0033Epoch 00019: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0033 - val_loss: 0.0066\n",
      "Epoch 20/20\n",
      "143104/143697 [============================>.] - ETA: 0s - loss: 0.0032Epoch 00020: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0032 - val_loss: 0.0066\n",
      "('Result on validation data: ', 1.819035942957274)\n",
      "fittingNN_with_EntityEmbedding\n",
      "Train on 143697 samples, validate on 7563 samples\n",
      "Epoch 1/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0106Epoch 00001: val_loss improved from inf to 0.00830, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 12s 81us/step - loss: 0.0106 - val_loss: 0.0083\n",
      "Epoch 2/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00002: val_loss improved from 0.00830 to 0.00746, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 3/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0068Epoch 00003: val_loss improved from 0.00746 to 0.00724, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 4/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0062Epoch 00004: val_loss improved from 0.00724 to 0.00709, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0062 - val_loss: 0.0071\n",
      "Epoch 5/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0057Epoch 00005: val_loss improved from 0.00709 to 0.00699, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0057 - val_loss: 0.0070\n",
      "Epoch 6/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0054Epoch 00006: val_loss improved from 0.00699 to 0.00670, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0054 - val_loss: 0.0067\n",
      "Epoch 7/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0050Epoch 00007: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0050 - val_loss: 0.0067\n",
      "Epoch 8/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0047Epoch 00008: val_loss improved from 0.00670 to 0.00666, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0047 - val_loss: 0.0067\n",
      "Epoch 9/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0045Epoch 00009: val_loss improved from 0.00666 to 0.00654, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0045 - val_loss: 0.0065\n",
      "Epoch 10/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0043Epoch 00010: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0043 - val_loss: 0.0067\n",
      "Epoch 11/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0041Epoch 00011: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0041 - val_loss: 0.0068\n",
      "Epoch 12/20\n",
      "143232/143697 [============================>.] - ETA: 0s - loss: 0.0040Epoch 00012: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 75us/step - loss: 0.0040 - val_loss: 0.0066\n",
      "Epoch 13/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0038Epoch 00013: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0038 - val_loss: 0.0066\n",
      "Epoch 14/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0037Epoch 00014: val_loss improved from 0.00654 to 0.00650, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0037 - val_loss: 0.0065\n",
      "Epoch 15/20\n",
      "142976/143697 [============================>.] - ETA: 0s - loss: 0.0036Epoch 00015: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0036 - val_loss: 0.0066\n",
      "Epoch 16/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0035Epoch 00016: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0035 - val_loss: 0.0065\n",
      "Epoch 17/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0034Epoch 00017: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0034 - val_loss: 0.0065\n",
      "Epoch 18/20\n",
      "143488/143697 [============================>.] - ETA: 0s - loss: 0.0033Epoch 00018: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0033 - val_loss: 0.0065\n",
      "Epoch 19/20\n",
      "143616/143697 [============================>.] - ETA: 0s - loss: 0.0033Epoch 00019: val_loss improved from 0.00650 to 0.00647, saving model to best_model_weights.hdf5\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0033 - val_loss: 0.0065\n",
      "Epoch 20/20\n",
      "143360/143697 [============================>.] - ETA: 0s - loss: 0.0032Epoch 00020: val_loss did not improve\n",
      "143697/143697 [==============================] - 11s 74us/step - loss: 0.0032 - val_loss: 0.0066\n",
      "('Result on validation data: ', 1.7698644018242928)\n"
     ]
    }
   ],
   "source": [
    "file_object.finding_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=file_object.get_best_model()\n",
    "file_object.update_best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### preprocess= file_object.get_preporcessed_file()\n",
    "preprocess.get_dim()\n",
    "pre_x= preprocess.get_pre_x()\n",
    "pre_y= preprocess.get_pre_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre_x : categorical to nominal convert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key= file_object.get_prediction_key()\n",
    "\n",
    "def categorical_to_nominal(df, variable, key):\n",
    "        # copythe code from preporcess.py\n",
    "        # self.df       self.key   self.variables\n",
    "        \n",
    "        for x in variable:\n",
    "\n",
    "            if (\"int\" in str(type(np.array(df[x])[1]))) or (\"float\" in str(type(np.array(df[x])[1]))) or (\"long\" in str(type(np.array(df[x])[1]))):\n",
    "\n",
    "                pass\n",
    "            else:\n",
    "                print(x)\n",
    "                df[[x]], key =nom_convert_int(df[[x]],x, key)\n",
    "              \n",
    "        print(\"categoricol to nominal done!\")\n",
    "        return df, key\n",
    "    \n",
    "def nom_convert_int(df,x, key):\n",
    "        x_c= np.array(df)\n",
    "        dic=key[x]\n",
    "        try:\n",
    "            value_temp=max(key[x].values())+1\n",
    "            print(len(x_c))\n",
    "            for j in range(len(x_c)):\n",
    "                try: \n",
    "                    x_c[j][0]=dic[x_c[j][0]]\n",
    "                except:\n",
    "                    dic[x_c[j][0]]= value_temp\n",
    "                    value_temp= value_temp+1\n",
    "                    x_c[j][0]=dic[x_c[j][0]]\n",
    "            key[x]= dic\n",
    "        except:\n",
    "            pass\n",
    "        return x_c , key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division\n",
      "1501\n",
      "Season\n",
      "1501\n",
      "Product Type\n",
      "1501\n",
      "Customer Type Description\n",
      "1501\n",
      "Customer #\n",
      "1501\n",
      "Label (Style Mst)\n",
      "1501\n",
      "Label Desc. (Order Detail)\n",
      "1501\n",
      "Category Description\n",
      "1501\n",
      "STYLE #\n",
      "1501\n",
      "Order Type Desc\n",
      "1501\n",
      "categoricol to nominal done!\n"
     ]
    }
   ],
   "source": [
    "pass_to_model, key[2]= categorical_to_nominal(pre_x, key[1], key[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.NN_with_EntityEmbedding at 0x7efe5acb45d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= file_object.get_best_model()\n",
    "mdl= model['NN_with_EntityEmbedding'][0]\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.19064999,  15.67213345,   3.22135758, ...,   2.94764113,\n",
       "         2.94764113,  69.61871338], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mdl.fit(pass_to_model[:5], pre_y[:5],preprocess.get_test_x , preprocess.get_test_y)\n",
    "guessed= mdl.guess(np.array(pass_to_model))\n",
    "guessed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ms8909/3.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotly.tools.set_credentials_file(username='ms8909', api_key='OOQ413hzFuXQFdeEbpJK')\n",
    "\n",
    "x_temp = []\n",
    "for i in range(len(guessed)):\n",
    "    x_temp.append(i)\n",
    "data = []\n",
    "trace1 = go.Scatter(\n",
    "        x=x_temp,\n",
    "        y=guessed,\n",
    "        mode='lines',\n",
    "        name='predicted Sales'\n",
    "        )\n",
    "data.append(trace1)\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "            x=x_temp,\n",
    "            y=pre_y,\n",
    "            mode='lines',\n",
    "            name='actual'\n",
    "        )\n",
    "\n",
    "data.append(trace2)\n",
    "\n",
    "fig = dict(data=data)\n",
    "py.iplot(fig, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31051\n",
      "41215.855484\n"
     ]
    }
   ],
   "source": [
    "print(sum(pre_y))\n",
    "print(sum(guessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  2,  2, ..., 12, 12, 12])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pass_to_model['Date Startmonthhh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_r={1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0, 10:0,11:0,12:0}\n",
    "dic_f={1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0, 10:0,11:0,12:0}\n",
    "count=0\n",
    "for i in np.array(pass_to_model['Date Startmonthhh']):\n",
    "    dic_r[i]= dic_r[i]+ pre_y[count]\n",
    "    dic_f[i]= dic_f[i]+ guessed[count]\n",
    "    count= count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_f= {1: 11400.522019743919,\n",
    " 2: 4068.8966212272644,\n",
    " 3: 4829.8860155344009,\n",
    " 4: 2695.2984634637833,\n",
    " 5: 4882.1530286073685,\n",
    " 6: 2869.8497329950333,\n",
    " 7: 1652.3222538232803,\n",
    " 8: 1419.0004802942276,\n",
    " 9: 2011.2766766548157,\n",
    " 10: 1789.8727869987488,\n",
    " 11: 1676.3972688913345,\n",
    " 12: 1920.3801357746124}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_r= {1: 8610,\n",
    " 2: 3141,\n",
    " 3: 3384,\n",
    " 4: 2576,\n",
    " 5: 4201,\n",
    " 6: 2277,\n",
    " 7: 1095,\n",
    " 8: 1531,\n",
    " 9: 1608,\n",
    " 10: 930,\n",
    " 11: 793,\n",
    " 12: 905}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ms8909/3.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly\n",
    "from plotly import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.tools.set_credentials_file(username='ms8909', api_key='OOQ413hzFuXQFdeEbpJK')\n",
    "\n",
    "x_temp = []\n",
    "for i in range(len(dic_f.values())):\n",
    "    x_temp.append(i)\n",
    "data = []\n",
    "trace1 = go.Scatter(\n",
    "        x=['jan', 'feb', 'mar', 'apr', 'may','june', 'july', 'aug', 'sep', 'oct', 'nov', 'dec'],\n",
    "        y=dic_f.values(),\n",
    "        mode='lines',\n",
    "        name='unitsshipped'\n",
    "        )\n",
    "data.append(trace1)\n",
    "\n",
    "fig = dict(data=data)\n",
    "py.iplot(fig, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ms8909/3.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly\n",
    "from plotly import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.tools.set_credentials_file(username='ms8909', api_key='OOQ413hzFuXQFdeEbpJK')\n",
    "\n",
    "x_temp = []\n",
    "for i in range(len(dic_f.values())):\n",
    "    x_temp.append(i)\n",
    "data = []\n",
    "trace1 = go.Scatter(\n",
    "        x=['jan', 'feb', 'mar', 'apr', 'may','june', 'july', 'aug', 'sep', 'oct', 'nov', 'dec'],\n",
    "        y=dic_f.values(),\n",
    "        mode='lines',\n",
    "        name='predicted UnitsShipped'\n",
    "        )\n",
    "data.append(trace1)\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "            x=['jan', 'feb', 'mar', 'apr', 'may','june', 'july', 'aug', 'sep', 'oct', 'nov', 'dec'],\n",
    "            y=dic_r.values(),\n",
    "            mode='lines',\n",
    "            name='actual UnitsShipped'\n",
    "        )\n",
    "\n",
    "data.append(trace2)\n",
    "\n",
    "fig = dict(data=data)\n",
    "py.iplot(fig, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
