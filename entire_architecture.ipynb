{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jul 29 00:20:11 2017\n",
    "\n",
    "@author: Muddassar Sharif\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "from models import *\n",
    "from preprocess  import *\n",
    "from graph import *\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "import datetime\n",
    "import json\n",
    "import dill\n",
    "\n",
    "import plotly\n",
    "from plotly import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "class Dashboard():\n",
    "\n",
    "    def __init__(self, address, y_var):\n",
    "        self.data_address= address\n",
    "#        self.models={'LinearModel':[LinearModel() for i in range(3)],'RF':[RF() for i in range(3)], 'SVM': [SVM() for i in range(3)], 'XGBoost':[XGBoost() for i in range(3)], 'HistricalMedian':[HistricalMedian() for i in range(3)], 'KNN':[KNN() for i in range(3)], 'NN_with_EntityEmbedding': [NN_with_EntityEmbedding() for i in range(3)], 'NN2_with_EntityEmbedding': [NN2_with_EntityEmbedding() for i in range(3)], 'NN': [NN() for i in range(3)] }\n",
    "        self.models={'NN_with_EntityEmbedding': [NN_with_EntityEmbedding() for i in range(3)]}\n",
    "  #      self.models={'NN_with_EntityEmbedding': [NN_with_EntityEmbedding() for i in range(3)], 'Xgboost': [XGBoost() for i in range(3)]}\n",
    "\n",
    "        self.best_model={}\n",
    "        self.y_variable=y_var\n",
    "        basename = \"preprocessing/preprocessing_object\"\n",
    "        suffix = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "        file_name = \"_\".join([basename, suffix]) + \".pickle\"  # e.g. 'mylogfile_120508_171442'\n",
    "        self.preprocess_p_file = file_name\n",
    "        suffix = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "        file_name = \"_\".join(['bestmodel/best_model', suffix]) + \".yaml\"  # e.g. 'mylogfile_120508_171442'\n",
    "        self.best_model_file= file_name\n",
    "        self.preprocess_object= preprocess()\n",
    "        self.x=None\n",
    "        self.preprocess_object.file_address(address)\n",
    "        self.mode= \"train\"\n",
    "        self.rows_info= {\"start\":0, \"end\":0}\n",
    "        self.graph_objects=[]\n",
    "        self.key= None\n",
    "        \n",
    "        \n",
    "    #save and load preprocess object \n",
    "    def save_preprocess_object(self):\n",
    "        with open(self.preprocess_p_file, 'wb') as f:\n",
    "            pickle.dump((self.preprocess_object), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def load_preprocess_object(self):\n",
    "        with open(self.preprocess_p_file, 'rb') as f:\n",
    "            self.preprocess_object= pickle.load(f)\n",
    "            \n",
    "            \n",
    "    def check_for_new_data(self):\n",
    "        \n",
    "#        try:\n",
    "            if len(pd.read_csv(self.data_address).index)== self.rows_info[\"end\"]:\n",
    "                print(\"No New Data added to the file\")\n",
    "            \n",
    "            else:\n",
    "            # time to train this baby : incremental learning \n",
    "                print(len(pd.read_csv(self.data_address).index), self.rows_info[\"end\"])\n",
    "                print(\"file has new data\")\n",
    "                self.new_data_transformation()\n",
    "#        except:\n",
    "#            print(\"some error with the file so retraining can not be done\")\n",
    "\n",
    "    \n",
    "    def test_train_switch(self, mde):\n",
    "        self.mode= mde\n",
    "        \n",
    "    def more_data(self, address):\n",
    "        self.data_address.append(address)\n",
    "        \n",
    "    def file_transformation(self):\n",
    "        \n",
    "        print(\"reading file\")\n",
    "\n",
    "        temp= self.preprocess_object.read_file(None, self.rows_info[\"start\"])  # just reading 10000 rows for testing right now\n",
    "        self.rows_info[\"end\"]= temp\n",
    "      \n",
    "        var= self.preprocess_object.variables()\n",
    "        print(var)\n",
    "\n",
    "        sel_var= self.preprocess_object.extract_variables(None)\n",
    "        print(sel_var) \n",
    "        \n",
    "        ans= self.preprocess_object.replace_nan(0)\n",
    "        print(ans)\n",
    "        \n",
    "        ans= self.preprocess_object.split_time()  # automate this process finding the data using\n",
    "        print(ans) \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        if self.mode == \"train\":  \n",
    "            ans= self.preprocess_object.Filterout(self.y_variable, 0)\n",
    "            ans= self.preprocess_object.choose_y(self.y_variable)\n",
    "\n",
    "            print(ans)\n",
    "            \n",
    "        \n",
    "            \n",
    "      \n",
    "        ans1= self.preprocess_object.category_to_nominal()\n",
    "        print(ans1)\n",
    "        dim= self.preprocess_object.calculate_dim()\n",
    "        print(dim)\n",
    "        self.preprocess_object.test_train_divide(.2)\n",
    "        self.x= self.preprocess_object.get_test_x()\n",
    "        \n",
    "\n",
    "        \n",
    "    # function to take care of training when new data comes in\n",
    "    def new_data_transformation(self):\n",
    "        \n",
    "        self.save_preprocess_object()\n",
    "        print(\"reading file\")\n",
    "        self.preprocess_object.save_data_and_y()    # this will backup previous data and y\n",
    "        temp= self.preprocess_object.read_file(None, self.rows_info[\"end\"])    # reading new data\n",
    "        \n",
    "        \n",
    "        \n",
    "        ans= self.preprocess_object.split_time()  # time variable taken care of changed from self.preprocess_object.get_time_var_name()\n",
    "        print(ans) \n",
    "\n",
    "        sel_var= self.preprocess_object.extract_variables(self.preprocess_object.get_s_variables())  # this will give an errot right now\n",
    "        print(sel_var) \n",
    "        \n",
    "        ans= self.preprocess_object.replace_nan(0)\n",
    "        print(ans)\n",
    "        \n",
    "\n",
    "        \n",
    "        if self.mode == \"train\":  \n",
    "            ans= self.preprocess_object.Filterout(self.y_variable, 0)\n",
    "            ans= self.preprocess_object.choose_y(self.y_variable)\n",
    "\n",
    "            print(ans)\n",
    "            \n",
    "            # preprocess function to divide the time between test and train\n",
    "            \n",
    "      \n",
    "        ans1= self.preprocess_object.category_to_nominal()                               # checked for errors and is working properly.\n",
    "        print(ans1)\n",
    "\n",
    "        self.preprocess_object.test_train_divide(.01)                                    # no need for this step as we are just training\n",
    "        self.save_preprocess_object()\n",
    "        self.train_best_model()\n",
    "        \n",
    "        self.rows_info[\"end\"] = temp\n",
    "        \n",
    "\n",
    "    def train_best_model(self):\n",
    "        for model_name in self.best_model.keys():\n",
    "                self.best_model[model_name][0].train()    # make a traiining function to just train the file\n",
    "        \n",
    "    \n",
    "\n",
    "   \n",
    "    def finding_best_model(self):\n",
    "        self.save_preprocess_object()\n",
    "        if self.best_model.keys()!=[]:\n",
    "            for model in self.best_model.keys():\n",
    "                print('fitting'+ str(model))\n",
    "                for i in range(len(self.best_model[model])):\n",
    "                    self.best_model[model][i].input(self.preprocess_p_file)\n",
    "        \n",
    "        else:\n",
    "            for model in self.models.keys():\n",
    "                for i in range(len(self.models[model])):\n",
    "                    print('fitting'+ str(model))\n",
    "                    self.models[model][i].input(self.preprocess_p_file)\n",
    "                    \n",
    "            self.update_best_model()\n",
    "\n",
    "\n",
    "    def update_best_model(self):\n",
    "        temp= [None,None, 1000000000000000000000000000000]\n",
    "        for i in self.models.keys():\n",
    "            for model in self.models[i]:\n",
    "                if model.get_results()<temp[2]:\n",
    "                    temp=[i, model, model.get_results]\n",
    "        self.best_model[temp[0]]=[temp[1]]\n",
    "        self.preprocess_object.save_data_and_y()\n",
    "        #self.save_preprocess_object()        \n",
    "        #self.preprocess_object= None   # now added to reduce the burden on the code\n",
    "\n",
    "    def get_prediction_key(self):\n",
    "        #self.load_preprocess_object()\n",
    "        key= self.preprocess_object.get_key_for_perdiction()\n",
    "        return [-1, key[0], key[1]]\n",
    "#        graph1= graph()\n",
    "#        self.graph_objects.append(graph1) \n",
    "#        self.graph_objects[-1].key_from_user(key)\n",
    "#        return [self.graph_objects.index(self.graph_objects[-1]), key[0], key[1]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def make_graph_object(self):\n",
    "#        self.load_preprocess_object()\n",
    "#        key= self.preprocess_object.get_key_for_perdiction()\n",
    "        graph1= graph()\n",
    "        self.graph_objects.append(graph1) \n",
    "        self.graph_objects[-1].key_from_user(self.key)\n",
    "        return [self.graph_objects.index(self.graph_objects[-1])]\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    def make_graph(self,k, user_in):\n",
    "        if user_in != None:\n",
    "            \n",
    "            self.graph_objects[k].user_input(user_in)\n",
    "       # return self.graph_objects[k].get_df_for_per()\n",
    "        y= self.best_model[self.best_model.keys()[0]][0].guess(np.array(self.graph_objects[k].get_df_for_per()))  \n",
    "        self.graph_objects[k].set_per_y(y)\n",
    "        return [self.graph_objects.index(self.graph_objects[k]), self.graph_objects[k].get_plot_data()]\n",
    "\n",
    "\n",
    "\n",
    "    def get_best_model(self):\n",
    "        return self.best_model\n",
    "    \n",
    "    def get_trained_models(self):\n",
    "        return self.models\n",
    "    \n",
    "    def get_preporcessed_file(self):\n",
    "        return self.preprocess_object\n",
    "\n",
    "    def save(self):\n",
    "        #save the best model\n",
    "        #s= json.dumps(self.best_model)\n",
    "\n",
    "        y = self.best_model[self.best_model.keys()[0]][0].guess(self.preprocess_object.get_test_x())\n",
    "        print(y)\n",
    "\n",
    "        plotly.tools.set_credentials_file(username='ms8909', api_key='OOQ413hzFuXQFdeEbpJK')\n",
    "\n",
    "        x_temp = []\n",
    "        for i in range(len(y)):\n",
    "            x_temp.append(i)\n",
    "        data = []\n",
    "        trace1 = go.Scatter(\n",
    "            x=x_temp,\n",
    "            y=y,\n",
    "            mode='lines',\n",
    "            name='predicted Sales'\n",
    "        )\n",
    "        data.append(trace1)\n",
    "\n",
    "        trace2 = go.Scatter(\n",
    "            x=x_temp,\n",
    "            y=self.preprocess_object.get_test_y(),\n",
    "            mode='lines',\n",
    "            name='actual'\n",
    "        )\n",
    "\n",
    "        data.append(trace2)\n",
    "\n",
    "        fig = dict(data=data)\n",
    "        py.iplot(fig, filename='line-mode')\n",
    "        with open(self.best_model_file, 'w') as f:\n",
    "            dill.dump(self.best_model, f)\n",
    "            #pickle.dump((self.best_model), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        return self.best_model[self.best_model.keys()[0]][0].get_results()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #implement changes on other models after one model is finalized.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save unction updated. \n",
    "# file name creation updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
